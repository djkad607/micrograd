{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd\n",
    "This is a micrograd engine. Micrograd is a small autograd engine. An autograd engine is used for automatic differentiation in tensors and gradients. Micrograd is essentially a small version of that. We shall start by importing all necessary libraries. Even though we are importing some libraries, the code for micrograd will be written from scratch. Later on, we might test out the autograd engine on PyTorch. The libraries we import are math and random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Objects\n",
    "A value object is a node in a computation graph here. Mathematically, one can think of it as simply a number. However, for the development of our micrograd architecture, we are to look at value objects as nodes in a computation graph. Here, we define functions for initialization, reproduction, addition, multliplication, exponentiation, reverse multiplication, true division, hyperbolic tangent function, exponentiation function and backward gradient computation. Apart from the hyperbolic tangent, exponentiation function and backward gradient computation, all other functions are special functions for Python. In the backward gradient function, we perform a topological sorting algorithm. This way, the computation graph gets arranged in the form of a linear chain, an array or simply a tensor. This helps in leveraging the GPU resources(which we don't have yet) and makes automatic gradient computation extremely convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, (self,), f\"**{other}\")\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other - 1) * out.grad # exercise :)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other ** -1\n",
    "    def __neg__(self): # - self\n",
    "        return self * - 1\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (- other)\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "        def _backward():\n",
    "            self.grad += (1 - t ** 2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # ??? exercise :)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "Layers in a computation graph are levels of depth in which one particular node is situated. A generic computation graph has an input layer, an output layer and hidden layers in between. The weights of the net reside between the layers. Mathematically, one can express the weights between two consecutive layers in the form of a matrix. As said earlier, a matrix is a two dimensional tensor. The goal of training a computation graph is to optimize these weight matrices and bias vectors to minimize a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n",
    "Now we define a computation graph. This one takes in three inputs and provides one output. It has three layers. The first two hidden layers consist of four neurons each and the the output layer, as said before, has one neuron. Mathematically, one can think of a computation graph as a highly abstract, non-linear transformation from an input space to an output space. $\\\\ ComputationGraph: \\mathbb{R}^i \\rightarrow \\mathbb{R}^o$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.34077904978630646)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.3409343871516124),\n",
       " Value(data=-0.06058318243490701),\n",
       " Value(data=-0.4441141795796997),\n",
       " Value(data=-0.6097174580145774),\n",
       " Value(data=-0.9662445362517325),\n",
       " Value(data=-0.2509148473917726),\n",
       " Value(data=-0.21027110652128989),\n",
       " Value(data=-0.4932680280729833),\n",
       " Value(data=-0.6464314457919778),\n",
       " Value(data=-0.9304389346354631),\n",
       " Value(data=-0.11727312704124149),\n",
       " Value(data=0.20275808745023216),\n",
       " Value(data=0.0332832427753742),\n",
       " Value(data=-0.46097424848899693),\n",
       " Value(data=-0.4974206561479164),\n",
       " Value(data=-0.4334330845091312),\n",
       " Value(data=-0.15350402567124877),\n",
       " Value(data=0.6616531611429193),\n",
       " Value(data=-0.18472958038807175),\n",
       " Value(data=-0.6461427288904391),\n",
       " Value(data=0.3499599277802974),\n",
       " Value(data=0.6206098125762132),\n",
       " Value(data=0.29633518093890254),\n",
       " Value(data=0.7335312948549997),\n",
       " Value(data=0.9595147488635791),\n",
       " Value(data=-0.7651786361425401),\n",
       " Value(data=0.6845235555215115),\n",
       " Value(data=-0.21742264746237905),\n",
       " Value(data=-0.8916808711473037),\n",
       " Value(data=0.8692453153760757),\n",
       " Value(data=0.43074570366625275),\n",
       " Value(data=-0.8640172401641775),\n",
       " Value(data=-0.8644226973569966),\n",
       " Value(data=0.33956793658178497),\n",
       " Value(data=-0.06376022382631152),\n",
       " Value(data=-0.861138867897155),\n",
       " Value(data=-0.2481337084918842),\n",
       " Value(data=-0.9379456653054745),\n",
       " Value(data=0.019848001799562587),\n",
       " Value(data=0.8448749287445021),\n",
       " Value(data=-0.061826045986450584)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation\n",
    "Let us consider a node in a computation graph. It takes inputs, scales them by their corresponding weights, adds them up, adds a bias, passes the weighted sum to an activation function and returns the result. An activation function is highly relevant here because without one, a model always behaves like a linear model regardless of the number of hidden layers. The activation function used here is the hyperbolic tangent function. Its range is the closed interval from - 1 to 1, or $[-1, 1]$.\n",
    "\n",
    "$tanh: \\mathbb{R} \\rightarrow [-1, 1] \\hspace{3em} tanh(t) = \\frac{e^{t} - e^{-t}}{e^{t} + e^{-t}} \\hspace{1em} \\forall t \\in \\mathbb{R} \\hspace{3em} tanh'(t) = 1 - tanh(t)^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: Value(data=5.983738136954173)\n",
      "Epoch: 1, Loss: Value(data=4.658961225912644)\n",
      "Epoch: 2, Loss: Value(data=3.7316404410036905)\n",
      "Epoch: 3, Loss: Value(data=3.0533340575654444)\n",
      "Epoch: 4, Loss: Value(data=2.6203498092356794)\n",
      "Epoch: 5, Loss: Value(data=2.274704760423538)\n",
      "Epoch: 6, Loss: Value(data=1.9248632192964452)\n",
      "Epoch: 7, Loss: Value(data=1.516975615195223)\n",
      "Epoch: 8, Loss: Value(data=1.0785238636176495)\n",
      "Epoch: 9, Loss: Value(data=0.7220719378115273)\n",
      "Epoch: 10, Loss: Value(data=0.5013577040845125)\n",
      "Epoch: 11, Loss: Value(data=0.37121601786125785)\n",
      "Epoch: 12, Loss: Value(data=0.290840156958421)\n",
      "Epoch: 13, Loss: Value(data=0.2375133056552385)\n",
      "Epoch: 14, Loss: Value(data=0.19996046197292663)\n",
      "Epoch: 15, Loss: Value(data=0.17225373199557326)\n",
      "Epoch: 16, Loss: Value(data=0.15104137465003115)\n",
      "Epoch: 17, Loss: Value(data=0.1343138022267933)\n",
      "Epoch: 18, Loss: Value(data=0.12080305385143722)\n",
      "Epoch: 19, Loss: Value(data=0.10967348329279518)\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum(((yout - ygt) ** 2 for ygt, yout in zip(ys, ypred)), Value(0.0))\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "    print(f\"Epoch: {k}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8885797465115218),\n",
       " Value(data=-0.8459486721432505),\n",
       " Value(data=-0.8059148532670184),\n",
       " Value(data=0.8370942843524493)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
